{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d30634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPUs visible: []\n",
      "\n",
      "Loading model...\n",
      "✓ Model loaded\n",
      "\n",
      "Rebuilding tokenizer (MATCH TRAINING)...\n",
      "✓ Total images with captions: 8091\n",
      "✗ Malformed lines skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 20:13:58.926119: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer rebuilt (training-matched)\n",
      "✓ Vocabulary size: 5000\n",
      "\n",
      "Running sanity test...\n",
      "\n",
      "Image: image.png\n",
      "  step 1: id=0 word=\n",
      "  step 2: id=0 word=\n",
      "  step 3: id=0 word=\n",
      "  step 4: id=0 word=\n",
      "  step 5: id=0 word=\n",
      "Caption: (no caption generated)\n",
      "\n",
      "✓ Inference finished (CPU only)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CPU-ONLY IMAGE CAPTION INFERENCE (TRAINING-MATCHED TOKENIZER)\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import random\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs visible:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG (MUST MATCH TRAINING)\n",
    "# =====================================================\n",
    "MODEL_PATH = \"/home/kavir/image_project/model_checkpoints/best_caption_model.keras\"\n",
    "CAPTIONS_FILE = \"/home/kavir/image_project/captions.txt\"\n",
    "TEST_IMAGE_DIR = \"/home/kavir/image_project/testImages\"\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SEQ_LENGTH = 20\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_DIM = 256\n",
    "\n",
    "# =====================================================\n",
    "# IMAGE LOADER (PNG/JPG SAFE)\n",
    "# =====================================================\n",
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)  # jpg/png safe\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "# =====================================================\n",
    "# CUSTOM TRANSFORMER LAYERS (MUST MATCH TRAINING)\n",
    "# =====================================================\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        x = self.norm1(inputs + attn_output)\n",
    "        return self.norm2(x + self.ffn(x))\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads=4, ff_dim=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.att1 = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.norm3 = layers.LayerNormalization()\n",
    "        self.out = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, enc_output):\n",
    "        x = self.embed(x)\n",
    "        x = self.norm1(x + self.att1(x, x))\n",
    "        x = self.norm2(x + self.att2(x, enc_output))\n",
    "        x = self.norm3(x + self.ffn(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# =====================================================\n",
    "# LOAD MODEL\n",
    "# =====================================================\n",
    "print(\"\\nLoading model...\")\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        \"TransformerEncoder\": TransformerEncoder,\n",
    "        \"TransformerDecoder\": TransformerDecoder\n",
    "    },\n",
    "    compile=False\n",
    ")\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# =====================================================\n",
    "# REBUILD TOKENIZER (MATCH TRAINING EXACTLY)\n",
    "# =====================================================\n",
    "print(\"\\nRebuilding tokenizer (MATCH TRAINING)...\")\n",
    "\n",
    "tokenizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=\"lower_and_strip_punctuation\"\n",
    ")\n",
    "\n",
    "# 1) Build captions dict exactly like training\n",
    "captions_dict = {}\n",
    "malformed_count = 0\n",
    "\n",
    "with open(CAPTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line or line.startswith(\"image,caption\"):\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\",\", 1)\n",
    "        if len(parts) < 2:\n",
    "            malformed_count += 1\n",
    "            continue\n",
    "\n",
    "        img = parts[0].strip()\n",
    "        cap = parts[1].strip()\n",
    "\n",
    "        if not img:\n",
    "            malformed_count += 1\n",
    "            continue\n",
    "        if len(cap) == 0:\n",
    "            continue\n",
    "\n",
    "        captions_dict.setdefault(img, []).append(cap)\n",
    "\n",
    "print(f\"✓ Total images with captions: {len(captions_dict)}\")\n",
    "print(f\"✗ Malformed lines skipped: {malformed_count}\")\n",
    "\n",
    "# 2) Build caption_seqs exactly like training\n",
    "caption_seqs = []\n",
    "for img, caps in captions_dict.items():  # dict insertion order preserved\n",
    "    for c in caps:\n",
    "        caption_seqs.append(str(\"sos \" + c + \" eos\"))\n",
    "\n",
    "# 3) Adapt tokenizer exactly like training\n",
    "adapt_dataset = tf.data.Dataset.from_tensor_slices(caption_seqs)\n",
    "tokenizer.adapt(adapt_dataset)\n",
    "\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "print(\"✓ Tokenizer rebuilt (training-matched)\")\n",
    "print(\"✓ Vocabulary size:\", len(vocab))\n",
    "\n",
    "FALLBACK_WORD = \"a\" if \"a\" in vocab_set else vocab[1]\n",
    "\n",
    "# =====================================================\n",
    "# CAPTION GENERATION\n",
    "# =====================================================\n",
    "def generate_caption(image_path, max_length=20, min_length=3, debug_steps=5):\n",
    "    img = load_image(image_path)\n",
    "    img = tf.expand_dims(img, 0)\n",
    "\n",
    "    caption = \"sos\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        seq = tokenizer([caption])\n",
    "        preds = model([img, seq], training=False)\n",
    "\n",
    "        next_id = int(tf.argmax(preds[0, -1]).numpy())\n",
    "        next_word = vocab[next_id]\n",
    "\n",
    "        # Debug: show first few predicted words\n",
    "        if i < debug_steps:\n",
    "            print(f\"  step {i+1}: id={next_id} word={next_word}\")\n",
    "\n",
    "        # Prevent early stop / unknown too early\n",
    "        if i < min_length and next_word in (\"eos\", \"[UNK]\"):\n",
    "            next_word = FALLBACK_WORD\n",
    "\n",
    "        if next_word == \"eos\":\n",
    "            break\n",
    "\n",
    "        if next_word == \"[UNK]\":\n",
    "            next_word = FALLBACK_WORD\n",
    "\n",
    "        caption += \" \" + next_word\n",
    "\n",
    "    result = caption.replace(\"sos \", \"\").strip()\n",
    "\n",
    "    if not result or result == \"sos\":\n",
    "        return \"(no caption generated)\"\n",
    "    return result\n",
    "\n",
    "# =====================================================\n",
    "# TEST\n",
    "# =====================================================\n",
    "print(\"\\nRunning sanity test...\")\n",
    "\n",
    "if not os.path.exists(TEST_IMAGE_DIR):\n",
    "    raise FileNotFoundError(f\"Test image folder not found: {TEST_IMAGE_DIR}\")\n",
    "\n",
    "images = [\n",
    "    os.path.join(TEST_IMAGE_DIR, f)\n",
    "    for f in os.listdir(TEST_IMAGE_DIR)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "if len(images) == 0:\n",
    "    raise RuntimeError(f\"No .jpg/.jpeg/.png images found in: {TEST_IMAGE_DIR}\")\n",
    "\n",
    "for img in random.sample(images, min(5, len(images))):\n",
    "    print(\"\\nImage:\", os.path.basename(img))\n",
    "    print(\"Caption:\", generate_caption(img))\n",
    "\n",
    "print(\"\\n✓ Inference finished (CPU only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d737b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be2349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
